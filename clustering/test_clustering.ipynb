{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN, HDBSCAN, OPTICS\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "\n",
    "# Important for conversion of lat-long based distance to meters\n",
    "RADIUS_OF_EARTH_AT_SPACE_NEEDLE = 6366.512563943 # km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def extract_yearly_data(df, dirname, base_filename, year_range):\n",
    "    dirpath = os.path.join(os.getcwd(), dirname)\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "    path = os.path.join(dirpath, base_filename)\n",
    "    for year in year_range:\n",
    "        year_df = df.loc[df[\"Year\"] == year]\n",
    "        year_df.to_csv(path + \"_year_\" + str(year) + \".csv\", index=False)\n",
    "    return\n",
    "\n",
    "\n",
    "def meters_to_hav(meters, R=RADIUS_OF_EARTH_AT_SPACE_NEEDLE):\n",
    "    \"\"\"Converts a distance in meters to haversine distance\"\"\"\n",
    "    hav = meters / (R * 1000)\n",
    "    return hav\n",
    "\n",
    "\n",
    "def get_range(df, col):\n",
    "    min_val = df[col].min()\n",
    "    max_val = df[col].max()\n",
    "    return min_val, max_val\n",
    "\n",
    "\n",
    "def linearly_interpolate(x, old_interval, new_interval):\n",
    "    a, b = old_interval\n",
    "    c, d = new_interval\n",
    "    new_x = c + (x - a) * ((d - c) / b - a)\n",
    "    return new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance metrics I was playing with\n",
    "def euclidean_dist(x, y):\n",
    "    return np.sqrt(np.sum((x - y) ** 2))\n",
    "\n",
    "\n",
    "def haversine_np(lon1, lat1, lon2, lat2, R=RADIUS_OF_EARTH_AT_SPACE_NEEDLE):\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = R * c\n",
    "    return km\n",
    "\n",
    "\n",
    "def linearized_haversine(a, b, R=RADIUS_OF_EARTH_AT_SPACE_NEEDLE):\n",
    "    a_rad = a * (2 * np.pi / 360)\n",
    "    b_rad = b * (2 * np.pi / 360)\n",
    "    x_1, y_1 = a_rad\n",
    "    x_2, y_2 = b_rad\n",
    "    delta_x = R * np.cos(y_1) * (x_2 - x_1)\n",
    "    delta_y = R * (y_2 - y_1)\n",
    "    return np.sqrt(delta_x**2 + delta_y**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the crime data\n",
    "crime_df = pd.read_csv(\"SPD_Crime_Data__2008-Present.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the crimes with missing time, category, or spatial data\n",
    "crime_df = crime_df.dropna(\n",
    "    axis=0,\n",
    "    how=\"any\",\n",
    "    subset=[\"Offense Start DateTime\", \"Offense Parent Group\", \"Longitude\", \"Latitude\"],\n",
    "    inplace=False,\n",
    ")\n",
    "\n",
    "# Filter crimes with wacky position data\n",
    "crime_df = crime_df.loc[(crime_df[\"Latitude\"] > 47) & (crime_df[\"Latitude\"] < 48) ]\n",
    "crime_df = crime_df.loc[(crime_df[\"Longitude\"] > -123) & (crime_df[\"Longitude\"] < -122)]\n",
    "\n",
    "# Drop report information to reduce data size\n",
    "crime_df = crime_df.drop(\n",
    "    axis=1, columns=[\"Report Number\", \"Report DateTime\"], inplace=False\n",
    ")\n",
    "\n",
    "# Convert time to datetime column\n",
    "crime_df[\"Offense Start DateTime\"] = pd.to_datetime(crime_df[\"Offense Start DateTime\"])\n",
    "\n",
    "# Extract offense year, month, day, and time of day\n",
    "crime_df['Year'] = crime_df['Offense Start DateTime'].dt.year\n",
    "crime_df['Month'] = crime_df['Offense Start DateTime'].dt.month.astype(int)\n",
    "crime_df['Day'] = crime_df['Offense Start DateTime'].dt.dayofweek.astype(int)\n",
    "crime_df['Time'] = crime_df['Offense Start DateTime'].dt.time\n",
    "\n",
    "# Filter to crimes since 2008\n",
    "crime_df = crime_df.loc[crime_df[\"Year\"] >= 2008]\n",
    "\n",
    "# Express lat and long in radians for haversine metric\n",
    "crime_df[\"long_rad\"] = crime_df[\"Longitude\"].apply(np.radians)\n",
    "crime_df[\"lat_rad\"] = crime_df[\"Latitude\"].apply(np.radians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Offense ID</th>\n",
       "      <th>Offense Start DateTime</th>\n",
       "      <th>Offense End DateTime</th>\n",
       "      <th>Group A B</th>\n",
       "      <th>Crime Against Category</th>\n",
       "      <th>Offense Parent Group</th>\n",
       "      <th>Offense</th>\n",
       "      <th>Offense Code</th>\n",
       "      <th>Precinct</th>\n",
       "      <th>Sector</th>\n",
       "      <th>...</th>\n",
       "      <th>MCPP</th>\n",
       "      <th>100 Block Address</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Time</th>\n",
       "      <th>long_rad</th>\n",
       "      <th>lat_rad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12605873663</td>\n",
       "      <td>2020-02-05 10:10:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>SOCIETY</td>\n",
       "      <td>DRUG/NARCOTIC OFFENSES</td>\n",
       "      <td>Drug/Narcotic Violations</td>\n",
       "      <td>35A</td>\n",
       "      <td>W</td>\n",
       "      <td>Q</td>\n",
       "      <td>...</td>\n",
       "      <td>MAGNOLIA</td>\n",
       "      <td>32XX BLOCK OF 23RD AVE W</td>\n",
       "      <td>-122.385974</td>\n",
       "      <td>47.649387</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10:10:00</td>\n",
       "      <td>-2.136038</td>\n",
       "      <td>0.831639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12605598696</td>\n",
       "      <td>2020-02-03 08:00:00</td>\n",
       "      <td>02/04/2020 08:00:00 AM</td>\n",
       "      <td>A</td>\n",
       "      <td>PROPERTY</td>\n",
       "      <td>LARCENY-THEFT</td>\n",
       "      <td>Theft of Motor Vehicle Parts or Accessories</td>\n",
       "      <td>23G</td>\n",
       "      <td>N</td>\n",
       "      <td>J</td>\n",
       "      <td>...</td>\n",
       "      <td>ROOSEVELT/RAVENNA</td>\n",
       "      <td>63XX BLOCK OF 5TH AVE NE</td>\n",
       "      <td>-122.323399</td>\n",
       "      <td>47.675118</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>08:00:00</td>\n",
       "      <td>-2.134946</td>\n",
       "      <td>0.832088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12605567653</td>\n",
       "      <td>2020-02-02 20:30:00</td>\n",
       "      <td>02/02/2020 09:30:00 PM</td>\n",
       "      <td>A</td>\n",
       "      <td>PROPERTY</td>\n",
       "      <td>ROBBERY</td>\n",
       "      <td>Robbery</td>\n",
       "      <td>120</td>\n",
       "      <td>N</td>\n",
       "      <td>U</td>\n",
       "      <td>...</td>\n",
       "      <td>ROOSEVELT/RAVENNA</td>\n",
       "      <td>26TH AVE NE / NE BLAKELEY ST</td>\n",
       "      <td>-122.299552</td>\n",
       "      <td>47.666384</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>20:30:00</td>\n",
       "      <td>-2.134530</td>\n",
       "      <td>0.831935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12605174036</td>\n",
       "      <td>2020-02-05 01:17:00</td>\n",
       "      <td>02/05/2020 02:21:00 AM</td>\n",
       "      <td>A</td>\n",
       "      <td>PROPERTY</td>\n",
       "      <td>DESTRUCTION/DAMAGE/VANDALISM OF PROPERTY</td>\n",
       "      <td>Destruction/Damage/Vandalism of Property</td>\n",
       "      <td>290</td>\n",
       "      <td>W</td>\n",
       "      <td>Q</td>\n",
       "      <td>...</td>\n",
       "      <td>MAGNOLIA</td>\n",
       "      <td>22XX BLOCK OF W RAYE ST</td>\n",
       "      <td>-122.384865</td>\n",
       "      <td>47.642927</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>01:17:00</td>\n",
       "      <td>-2.136019</td>\n",
       "      <td>0.831526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12605081469</td>\n",
       "      <td>2020-02-05 00:51:21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "      <td>SOCIETY</td>\n",
       "      <td>DRIVING UNDER THE INFLUENCE</td>\n",
       "      <td>Driving Under the Influence</td>\n",
       "      <td>90D</td>\n",
       "      <td>N</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>BALLARD SOUTH</td>\n",
       "      <td>NW 46TH ST / 8TH AVE NW</td>\n",
       "      <td>-122.366195</td>\n",
       "      <td>47.662193</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>00:51:21</td>\n",
       "      <td>-2.135693</td>\n",
       "      <td>0.831862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Offense ID Offense Start DateTime    Offense End DateTime Group A B  \\\n",
       "0  12605873663    2020-02-05 10:10:00                     NaN         A   \n",
       "1  12605598696    2020-02-03 08:00:00  02/04/2020 08:00:00 AM         A   \n",
       "2  12605567653    2020-02-02 20:30:00  02/02/2020 09:30:00 PM         A   \n",
       "3  12605174036    2020-02-05 01:17:00  02/05/2020 02:21:00 AM         A   \n",
       "4  12605081469    2020-02-05 00:51:21                     NaN         B   \n",
       "\n",
       "  Crime Against Category                      Offense Parent Group  \\\n",
       "0                SOCIETY                    DRUG/NARCOTIC OFFENSES   \n",
       "1               PROPERTY                             LARCENY-THEFT   \n",
       "2               PROPERTY                                   ROBBERY   \n",
       "3               PROPERTY  DESTRUCTION/DAMAGE/VANDALISM OF PROPERTY   \n",
       "4                SOCIETY               DRIVING UNDER THE INFLUENCE   \n",
       "\n",
       "                                       Offense Offense Code Precinct Sector  \\\n",
       "0                     Drug/Narcotic Violations          35A        W      Q   \n",
       "1  Theft of Motor Vehicle Parts or Accessories          23G        N      J   \n",
       "2                                      Robbery          120        N      U   \n",
       "3     Destruction/Damage/Vandalism of Property          290        W      Q   \n",
       "4                  Driving Under the Influence          90D        N      B   \n",
       "\n",
       "   ...               MCPP             100 Block Address   Longitude  \\\n",
       "0  ...           MAGNOLIA      32XX BLOCK OF 23RD AVE W -122.385974   \n",
       "1  ...  ROOSEVELT/RAVENNA      63XX BLOCK OF 5TH AVE NE -122.323399   \n",
       "2  ...  ROOSEVELT/RAVENNA  26TH AVE NE / NE BLAKELEY ST -122.299552   \n",
       "3  ...           MAGNOLIA       22XX BLOCK OF W RAYE ST -122.384865   \n",
       "4  ...      BALLARD SOUTH       NW 46TH ST / 8TH AVE NW -122.366195   \n",
       "\n",
       "    Latitude  Year  Month  Day      Time  long_rad   lat_rad  \n",
       "0  47.649387  2020      2    2  10:10:00 -2.136038  0.831639  \n",
       "1  47.675118  2020      2    0  08:00:00 -2.134946  0.832088  \n",
       "2  47.666384  2020      2    6  20:30:00 -2.134530  0.831935  \n",
       "3  47.642927  2020      2    2  01:17:00 -2.136019  0.831526  \n",
       "4  47.662193  2020      2    2  00:51:21 -2.135693  0.831862  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crime_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create yearly extracts if you want\n",
    "extract_yearly_data(crime_df, \"yearly_extracts\", \"SPD_Crime_Data\", range(2008, 2025))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data\n",
    "crime_df.to_csv(\"cleaned_SPD_Crime_Data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the cleaned data\n",
    "crime_df = pd.read_csv(\"cleaned_SPD_Crime_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking range of lat and long columns\n",
    "min_long, max_long = get_range(crime_df, \"Longitude\")\n",
    "min_lat, max_lat = get_range(crime_df, \"Latitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15724.679469803265, 36253.28281720292)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_range = 1000 * haversine_np(min_long, min_lat, max_long, min_lat)\n",
    "y_range = 1000 * haversine_np(min_long, min_lat, min_long, max_lat)\n",
    "\n",
    "x_range, y_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15724.68421289776, 36253.28281720292)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A linearized version introduces very little error, if we need it\n",
    "x_range_lin = 1000 * linearized_haversine(np.array([min_long, min_lat]), np.array([max_long, min_lat]))\n",
    "y_range_lin = 1000 * linearized_haversine(np.array([min_long, min_lat]), np.array([min_long, max_lat]))\n",
    "\n",
    "x_range_lin, y_range_lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Offense ID</th>\n",
       "      <th>Offense Start DateTime</th>\n",
       "      <th>Offense End DateTime</th>\n",
       "      <th>Group A B</th>\n",
       "      <th>Crime Against Category</th>\n",
       "      <th>Offense Parent Group</th>\n",
       "      <th>Offense</th>\n",
       "      <th>Offense Code</th>\n",
       "      <th>Precinct</th>\n",
       "      <th>Sector</th>\n",
       "      <th>...</th>\n",
       "      <th>MCPP</th>\n",
       "      <th>100 Block Address</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Time</th>\n",
       "      <th>long_rad</th>\n",
       "      <th>lat_rad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>974994</th>\n",
       "      <td>41318760607</td>\n",
       "      <td>2023-01-18 00:00:00</td>\n",
       "      <td>01/18/2023 11:59:00 PM</td>\n",
       "      <td>A</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>ASSAULT OFFENSES</td>\n",
       "      <td>Intimidation</td>\n",
       "      <td>13C</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>...</td>\n",
       "      <td>CAPITOL HILL</td>\n",
       "      <td>6XX BLOCK OF HARVARD AVE E</td>\n",
       "      <td>-122.321983</td>\n",
       "      <td>47.624694</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>-2.134921</td>\n",
       "      <td>0.831208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975041</th>\n",
       "      <td>41315958431</td>\n",
       "      <td>2023-01-30 10:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "      <td>SOCIETY</td>\n",
       "      <td>TRESPASS OF REAL PROPERTY</td>\n",
       "      <td>Trespass of Real Property</td>\n",
       "      <td>90J</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>CAPITOL HILL</td>\n",
       "      <td>14XX BLOCK OF E HOWELL ST</td>\n",
       "      <td>-122.313516</td>\n",
       "      <td>47.617626</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10:30:00</td>\n",
       "      <td>-2.134774</td>\n",
       "      <td>0.831084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975092</th>\n",
       "      <td>41312435583</td>\n",
       "      <td>2023-01-29 21:40:00</td>\n",
       "      <td>01/29/2023 10:12:00 PM</td>\n",
       "      <td>B</td>\n",
       "      <td>SOCIETY</td>\n",
       "      <td>DRIVING UNDER THE INFLUENCE</td>\n",
       "      <td>Driving Under the Influence</td>\n",
       "      <td>90D</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>...</td>\n",
       "      <td>CAPITOL HILL</td>\n",
       "      <td>14XX BLOCK OF 11TH AVE</td>\n",
       "      <td>-122.318132</td>\n",
       "      <td>47.613513</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>21:40:00</td>\n",
       "      <td>-2.134854</td>\n",
       "      <td>0.831013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975178</th>\n",
       "      <td>41288788167</td>\n",
       "      <td>2023-01-28 19:20:00</td>\n",
       "      <td>01/29/2023 03:20:00 AM</td>\n",
       "      <td>A</td>\n",
       "      <td>PROPERTY</td>\n",
       "      <td>LARCENY-THEFT</td>\n",
       "      <td>Theft From Building</td>\n",
       "      <td>23D</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>...</td>\n",
       "      <td>CAPITOL HILL</td>\n",
       "      <td>1XX BLOCK OF 10TH AVE E</td>\n",
       "      <td>-122.319530</td>\n",
       "      <td>47.619333</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>19:20:00</td>\n",
       "      <td>-2.134879</td>\n",
       "      <td>0.831114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975187</th>\n",
       "      <td>41288762898</td>\n",
       "      <td>2023-01-28 02:00:00</td>\n",
       "      <td>01/28/2023 02:01:00 AM</td>\n",
       "      <td>A</td>\n",
       "      <td>PROPERTY</td>\n",
       "      <td>LARCENY-THEFT</td>\n",
       "      <td>Theft From Building</td>\n",
       "      <td>23D</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>...</td>\n",
       "      <td>CAPITOL HILL</td>\n",
       "      <td>15XX BLOCK OF 11TH AVE</td>\n",
       "      <td>-122.318150</td>\n",
       "      <td>47.614674</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>02:00:00</td>\n",
       "      <td>-2.134854</td>\n",
       "      <td>0.831033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Offense ID Offense Start DateTime    Offense End DateTime Group A B  \\\n",
       "974994  41318760607    2023-01-18 00:00:00  01/18/2023 11:59:00 PM         A   \n",
       "975041  41315958431    2023-01-30 10:30:00                     NaN         B   \n",
       "975092  41312435583    2023-01-29 21:40:00  01/29/2023 10:12:00 PM         B   \n",
       "975178  41288788167    2023-01-28 19:20:00  01/29/2023 03:20:00 AM         A   \n",
       "975187  41288762898    2023-01-28 02:00:00  01/28/2023 02:01:00 AM         A   \n",
       "\n",
       "       Crime Against Category         Offense Parent Group  \\\n",
       "974994                 PERSON             ASSAULT OFFENSES   \n",
       "975041                SOCIETY    TRESPASS OF REAL PROPERTY   \n",
       "975092                SOCIETY  DRIVING UNDER THE INFLUENCE   \n",
       "975178               PROPERTY                LARCENY-THEFT   \n",
       "975187               PROPERTY                LARCENY-THEFT   \n",
       "\n",
       "                            Offense Offense Code Precinct Sector  ...  \\\n",
       "974994                 Intimidation          13C        E      E  ...   \n",
       "975041    Trespass of Real Property          90J        E      C  ...   \n",
       "975092  Driving Under the Influence          90D        E      E  ...   \n",
       "975178          Theft From Building          23D        E      E  ...   \n",
       "975187          Theft From Building          23D        E      E  ...   \n",
       "\n",
       "                MCPP           100 Block Address   Longitude   Latitude  Year  \\\n",
       "974994  CAPITOL HILL  6XX BLOCK OF HARVARD AVE E -122.321983  47.624694  2023   \n",
       "975041  CAPITOL HILL   14XX BLOCK OF E HOWELL ST -122.313516  47.617626  2023   \n",
       "975092  CAPITOL HILL      14XX BLOCK OF 11TH AVE -122.318132  47.613513  2023   \n",
       "975178  CAPITOL HILL     1XX BLOCK OF 10TH AVE E -122.319530  47.619333  2023   \n",
       "975187  CAPITOL HILL      15XX BLOCK OF 11TH AVE -122.318150  47.614674  2023   \n",
       "\n",
       "        Month  Day      Time  long_rad   lat_rad  \n",
       "974994      1    2  00:00:00 -2.134921  0.831208  \n",
       "975041      1    0  10:30:00 -2.134774  0.831084  \n",
       "975092      1    6  21:40:00 -2.134854  0.831013  \n",
       "975178      1    5  19:20:00 -2.134879  0.831114  \n",
       "975187      1    5  02:00:00 -2.134854  0.831033  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test runs on cap hill in 2023\n",
    "cap_hill_23 = crime_df.loc[\n",
    "    (crime_df[\"Year\"] == 2023) & (crime_df[\"MCPP\"] == \"CAPITOL HILL\")\n",
    "]\n",
    "cap_hill_23.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0.        , 1009.45075617, 1275.44504381, ...,  343.80890441,\n",
       "         343.80890441,  341.87047085],\n",
       "       [1009.45075617,    0.        ,  573.07081297, ..., 1266.67123412,\n",
       "        1266.67123412, 1114.80616355],\n",
       "       [1275.44504381,  573.07081297,    0.        , ..., 1599.70320173,\n",
       "        1599.70320173, 1220.48001123],\n",
       "       ...,\n",
       "       [ 343.80890441, 1266.67123412, 1599.70320173, ...,    0.        ,\n",
       "           0.        ,  618.30479661],\n",
       "       [ 343.80890441, 1266.67123412, 1599.70320173, ...,    0.        ,\n",
       "           0.        ,  618.30479661],\n",
       "       [ 341.87047085, 1114.80616355, 1220.48001123, ...,  618.30479661,\n",
       "         618.30479661,    0.        ]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test precomputing distance matrix---it's fast, which is good\n",
    "latlong = cap_hill_23[[\"lat_rad\", \"long_rad\"]].to_numpy()\n",
    "distances = haversine_distances(latlong)\n",
    "distances * (RADIUS_OF_EARTH_AT_SPACE_NEEDLE * 1000) # Approx distance in meters between crimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping of crime types\n",
    "1. We need to group the crime types somehow. I haven't done it yet though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some interest groups in the future\n",
    "assault_group = [\"ASSAULT OFFENSES\"]\n",
    "theft_group = [\"LARCENY-THEFT\", \"BURGLARY/BREAKING&ENTERING\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Offense ID', 'Offense Start DateTime', 'Offense End DateTime',\n",
       "       'Group A B', 'Crime Against Category', 'Offense Parent Group',\n",
       "       'Offense', 'Offense Code', 'Precinct', 'Sector', 'Beat', 'MCPP',\n",
       "       '100 Block Address', 'Longitude', 'Latitude', 'Year', 'Month', 'Day',\n",
       "       'Time', 'long_rad', 'lat_rad'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crime_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan\n",
    "1. GridSearch---store every labeling in output dataframe for comparison in Tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([72, 65, 62, 61, 55, 47, 45, 43, 41, 38, 37, 35, 33, 32, 32, 31, 31,\n",
       "       29, 25, 23])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def top_blocks(crime_df, n):\n",
    "    top_blocks = crime_df.groupby(['Latitude', 'Longitude'])\\\n",
    "        .size()\\\n",
    "        .to_frame(name='count')\\\n",
    "        .reset_index()\\\n",
    "        .sort_values(by=['count'], ascending=False)\\\n",
    "        .head(n)\n",
    "    return top_blocks[\"count\"].values\n",
    "\n",
    "top_blocks(cap_hill_23, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001570718646914388"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meters_to_hav(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dbscans(X, df, core_threshes, eps_meters=125):\n",
    "    \"\"\"DBSCAN grid search -> df + summaries\n",
    "\n",
    "    Labels stored in output. Write to disk for comparison in Tableau.\n",
    "    We need to fix epsilon small (100 meters) to constrain cluster size.\"\"\"\n",
    "    output_df = df.copy()\n",
    "    run_summaries = []\n",
    "\n",
    "    total_crime = X.shape[0]\n",
    "    eps = meters_to_hav(eps_meters)\n",
    "\n",
    "    for min_samples in core_threshes:\n",
    "        print(f\"DBSCAN clustering with eps={eps_meters}m, min_samples={min_samples}\")\n",
    "        # Cluster the data and extract the labels\n",
    "        colname = \"db_labs_eps=\" + str(eps_meters) + \"_ms=\" + str(min_samples)\n",
    "        db = DBSCAN(eps=eps, min_samples=min_samples, metric=\"haversine\").fit(X)\n",
    "        labels_db = db.labels_\n",
    "        # Count clusters and noise\n",
    "        num_clusters_ = len(set(labels_db)) - (1 if -1 in labels_db else 0)\n",
    "        num_noise_ = list(labels_db).count(-1)\n",
    "        percent_clustered_ = (total_crime - num_noise_) / total_crime\n",
    "        print(\"Estimated number of clusters: %d\" % num_clusters_)\n",
    "        print(\"Estimated number of noise points: %d\" % num_noise_)\n",
    "        print(f\"Estimated percentage of crime captured: {percent_clustered_}\\n\")\n",
    "        summary = {\n",
    "            \"type\": \"DBSCAN\",\n",
    "            \"model\": db,\n",
    "            \"params\": (eps_meters, min_samples),\n",
    "            \"num_clusters\": num_clusters_,\n",
    "            \"num_noise\": num_noise_,\n",
    "            \"percent_clustered\": percent_clustered_,\n",
    "        }\n",
    "        # Save the labels in the output df\n",
    "        run_summaries.append(summary)\n",
    "        output_df[colname] = labels_db\n",
    "    return output_df, run_summaries\n",
    "\n",
    "\n",
    "def run_optics(X, df, core_threshes, max_eps_meters=200):\n",
    "    \"\"\"OPTICS grid search -> df + summaries\n",
    "\n",
    "    Labels stored in output df. Write to disk for comparison in Tableau.\n",
    "    Similarly, we need to fix a small neighborhood size to limit size.\"\"\"\n",
    "    output_df = df.copy()\n",
    "    run_summaries = []\n",
    "\n",
    "    total_crime = X.shape[0]\n",
    "    max_eps = meters_to_hav(max_eps_meters)\n",
    "\n",
    "    for min_samples in core_threshes:\n",
    "        print(\n",
    "            f\"OPTICS clustering with max_eps={max_eps_meters}m, min_samples={min_samples}\"\n",
    "        )\n",
    "        # Cluster the data and extract the labels\n",
    "        colname = (\n",
    "            \"op_labs_eps=\"\n",
    "            + str(max_eps_meters)\n",
    "            + \"_ms=\"\n",
    "            + str(min_samples)\n",
    "        )\n",
    "        op = OPTICS(\n",
    "            max_eps=max_eps,\n",
    "            min_samples=min_samples,\n",
    "            metric=\"haversine\",\n",
    "            cluster_method=\"dbscan\",\n",
    "        ).fit(X)\n",
    "        labels_op = op.labels_\n",
    "        # Count clusters and noise\n",
    "        num_clusters_ = len(set(labels_op)) - (1 if -1 in labels_op else 0)\n",
    "        num_noise_ = list(labels_op).count(-1)\n",
    "        percent_clustered_ = (total_crime - num_noise_) / total_crime\n",
    "        print(\"Estimated number of clusters: %d\" % num_clusters_)\n",
    "        print(\"Estimated number of noise points: %d\" % num_noise_)\n",
    "        print(f\"Estimated percentage of crime captured: {percent_clustered_}\\n\")\n",
    "        summary = {\n",
    "            \"type\": \"OPTICS\",\n",
    "            \"model\": op,\n",
    "            \"params\": (max_eps_meters, min_samples),\n",
    "            \"num_clusters\": num_clusters_,\n",
    "            \"num_noise\": num_noise_,\n",
    "            \"percent_clustered\": percent_clustered_,\n",
    "        }\n",
    "        # Save labels in the output df\n",
    "        run_summaries.append(summary)\n",
    "        output_df[colname] = labels_op\n",
    "    return output_df, run_summaries\n",
    "\n",
    "\n",
    "def run_hdbscans(X, df, core_threshes, min_cluster_size, cluster_selection_eps_meters=100):\n",
    "    \"\"\"OPTICS grid search -> df + summaries\n",
    "\n",
    "    Labels stored in output df. Write to disk for comparison in Tableau.\n",
    "    Similarly, we need to fix a small neighborhood size to limit size.\"\"\"\n",
    "    output_df = df.copy()\n",
    "    run_summaries = []\n",
    "\n",
    "    total_crime = X.shape[0]\n",
    "    cluster_selection_eps = meters_to_hav(cluster_selection_eps_meters)\n",
    "\n",
    "    for min_samples in core_threshes:\n",
    "        print(\n",
    "            f\"HDBSCAN clustering with max_eps={cluster_selection_eps_meters}m, min_samples={min_samples}\"\n",
    "        )\n",
    "        # Cluster the data and extract the labels\n",
    "        colname = (\n",
    "            \"hdb_labs_eps=\"\n",
    "            + str(cluster_selection_eps_meters)\n",
    "            + \"_ms=\"\n",
    "            + str(min_samples)\n",
    "        )\n",
    "        hdb = HDBSCAN(\n",
    "            cluster_selection_epsilon=cluster_selection_eps,\n",
    "            min_samples=min_samples,\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            metric=\"haversine\",\n",
    "            store_centers=\"centroid\",\n",
    "        ).fit(X)\n",
    "        labels_hdb_ = hdb.labels_\n",
    "        # Count clusters and noise\n",
    "        num_clusters_ = len(set(labels_hdb_)) - (1 if -1 in labels_hdb_ else 0)\n",
    "        num_noise_ = list(labels_hdb_).count(-1)\n",
    "        percent_clustered_ = (total_crime - num_noise_) / total_crime\n",
    "        print(\"Estimated number of clusters: %d\" % num_clusters_)\n",
    "        print(\"Estimated number of noise points: %d\" % num_noise_)\n",
    "        print(f\"Estimated percentage of crime captured: {percent_clustered_}\\n\")\n",
    "        summary = {\n",
    "            \"type\": \"HDBSSCAN\",\n",
    "            \"model\": hdb,\n",
    "            \"params\": (cluster_selection_eps_meters, min_samples),\n",
    "            \"num_clusters\": num_clusters_,\n",
    "            \"num_noise\": num_noise_,\n",
    "            \"percent_clustered\": percent_clustered_,\n",
    "        }\n",
    "        # Save labels in the output df\n",
    "        run_summaries.append(summary)\n",
    "        output_df[colname] = labels_hdb_\n",
    "    return output_df, run_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radians for haversine\n",
    "X = cap_hill_23[[\"lat_rad\", \"long_rad\"]]\n",
    "# Taking top N ensures these blocks will be core points\n",
    "core_threshes = np.flip(np.unique(top_blocks(cap_hill_23, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBSCAN clustering with eps=100m, min_samples=102\n",
      "Estimated number of clusters: 4\n",
      "Estimated number of noise points: 1679\n",
      "Estimated percentage of crime captured: 0.5585064422824086\n",
      "\n",
      "DBSCAN clustering with eps=100m, min_samples=95\n",
      "Estimated number of clusters: 5\n",
      "Estimated number of noise points: 1514\n",
      "Estimated percentage of crime captured: 0.6018932421772285\n",
      "\n",
      "DBSCAN clustering with eps=100m, min_samples=92\n",
      "Estimated number of clusters: 6\n",
      "Estimated number of noise points: 1346\n",
      "Estimated percentage of crime captured: 0.6460688929792269\n",
      "\n",
      "DBSCAN clustering with eps=100m, min_samples=91\n",
      "Estimated number of clusters: 6\n",
      "Estimated number of noise points: 1346\n",
      "Estimated percentage of crime captured: 0.6460688929792269\n",
      "\n",
      "DBSCAN clustering with eps=100m, min_samples=85\n",
      "Estimated number of clusters: 6\n",
      "Estimated number of noise points: 1137\n",
      "Estimated percentage of crime captured: 0.7010255061793321\n",
      "\n",
      "DBSCAN clustering with eps=100m, min_samples=77\n",
      "Estimated number of clusters: 5\n",
      "Estimated number of noise points: 925\n",
      "Estimated percentage of crime captured: 0.7567709702866159\n",
      "\n",
      "DBSCAN clustering with eps=100m, min_samples=75\n",
      "Estimated number of clusters: 6\n",
      "Estimated number of noise points: 876\n",
      "Estimated percentage of crime captured: 0.7696555351038654\n",
      "\n",
      "DBSCAN clustering with eps=100m, min_samples=73\n",
      "Estimated number of clusters: 6\n",
      "Estimated number of noise points: 767\n",
      "Estimated percentage of crime captured: 0.7983171180646857\n",
      "\n",
      "DBSCAN clustering with eps=100m, min_samples=71\n",
      "Estimated number of clusters: 4\n",
      "Estimated number of noise points: 767\n",
      "Estimated percentage of crime captured: 0.7983171180646857\n",
      "\n",
      "DBSCAN clustering with eps=100m, min_samples=68\n",
      "Estimated number of clusters: 3\n",
      "Estimated number of noise points: 724\n",
      "Estimated percentage of crime captured: 0.8096239810675783\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dbscans_df, db_summaries = run_dbscans(X, cap_hill_23, core_threshes, eps_meters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTICS clustering with max_eps=100m, min_samples=72\n",
      "Estimated number of clusters: 6\n",
      "Estimated number of noise points: 860\n",
      "Estimated percentage of crime captured: 0.7738627399421509\n",
      "\n",
      "OPTICS clustering with max_eps=100m, min_samples=65\n",
      "Estimated number of clusters: 3\n",
      "Estimated number of noise points: 742\n",
      "Estimated percentage of crime captured: 0.804890875624507\n",
      "\n",
      "OPTICS clustering with max_eps=100m, min_samples=62\n",
      "Estimated number of clusters: 3\n",
      "Estimated number of noise points: 664\n",
      "Estimated percentage of crime captured: 0.8254009992111491\n",
      "\n",
      "OPTICS clustering with max_eps=100m, min_samples=61\n",
      "Estimated number of clusters: 3\n",
      "Estimated number of noise points: 664\n",
      "Estimated percentage of crime captured: 0.8254009992111491\n",
      "\n",
      "OPTICS clustering with max_eps=100m, min_samples=55\n",
      "Estimated number of clusters: 3\n",
      "Estimated number of noise points: 599\n",
      "Estimated percentage of crime captured: 0.8424927688666842\n",
      "\n",
      "OPTICS clustering with max_eps=100m, min_samples=47\n",
      "Estimated number of clusters: 4\n",
      "Estimated number of noise points: 474\n",
      "Estimated percentage of crime captured: 0.8753615566657902\n",
      "\n",
      "OPTICS clustering with max_eps=100m, min_samples=45\n",
      "Estimated number of clusters: 4\n",
      "Estimated number of noise points: 456\n",
      "Estimated percentage of crime captured: 0.8800946621088614\n",
      "\n",
      "OPTICS clustering with max_eps=100m, min_samples=43\n",
      "Estimated number of clusters: 4\n",
      "Estimated number of noise points: 439\n",
      "Estimated percentage of crime captured: 0.8845648172495398\n",
      "\n",
      "OPTICS clustering with max_eps=100m, min_samples=41\n",
      "Estimated number of clusters: 4\n",
      "Estimated number of noise points: 419\n",
      "Estimated percentage of crime captured: 0.8898238232973968\n",
      "\n",
      "OPTICS clustering with max_eps=100m, min_samples=38\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 362\n",
      "Estimated percentage of crime captured: 0.9048119905337891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optics_df, op_summaries = run_optics(X, cap_hill_23, core_threshes, max_eps_meters=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDBSCAN clustering with max_eps=100m, min_samples=72\n",
      "Estimated number of clusters: 5\n",
      "Estimated number of noise points: 1526\n",
      "Estimated percentage of crime captured: 0.5987378385485144\n",
      "\n",
      "HDBSCAN clustering with max_eps=100m, min_samples=65\n",
      "Estimated number of clusters: 5\n",
      "Estimated number of noise points: 1099\n",
      "Estimated percentage of crime captured: 0.7110176176702603\n",
      "\n",
      "HDBSCAN clustering with max_eps=100m, min_samples=62\n",
      "Estimated number of clusters: 6\n",
      "Estimated number of noise points: 1037\n",
      "Estimated percentage of crime captured: 0.7273205364186169\n",
      "\n",
      "HDBSCAN clustering with max_eps=100m, min_samples=61\n",
      "Estimated number of clusters: 6\n",
      "Estimated number of noise points: 991\n",
      "Estimated percentage of crime captured: 0.7394162503286879\n",
      "\n",
      "HDBSCAN clustering with max_eps=100m, min_samples=55\n",
      "Estimated number of clusters: 5\n",
      "Estimated number of noise points: 1080\n",
      "Estimated percentage of crime captured: 0.7160136734157244\n",
      "\n",
      "HDBSCAN clustering with max_eps=100m, min_samples=47\n",
      "Estimated number of clusters: 7\n",
      "Estimated number of noise points: 795\n",
      "Estimated percentage of crime captured: 0.790954509597686\n",
      "\n",
      "HDBSCAN clustering with max_eps=100m, min_samples=45\n",
      "Estimated number of clusters: 6\n",
      "Estimated number of noise points: 784\n",
      "Estimated percentage of crime captured: 0.7938469629240074\n",
      "\n",
      "HDBSCAN clustering with max_eps=100m, min_samples=43\n",
      "Estimated number of clusters: 5\n",
      "Estimated number of noise points: 672\n",
      "Estimated percentage of crime captured: 0.8232973967920063\n",
      "\n",
      "HDBSCAN clustering with max_eps=100m, min_samples=41\n",
      "Estimated number of clusters: 4\n",
      "Estimated number of noise points: 739\n",
      "Estimated percentage of crime captured: 0.8056797265316855\n",
      "\n",
      "HDBSCAN clustering with max_eps=100m, min_samples=38\n",
      "Estimated number of clusters: 5\n",
      "Estimated number of noise points: 605\n",
      "Estimated percentage of crime captured: 0.8409150670523271\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hdbscans_df, hdb_summaries = run_hdbscans(X, cap_hill_23, core_threshes, cluster_selection_eps_meters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting time windows\n",
    "1. Need to extract endpoints of the form (start_month, start_year), (end_month, end_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Offense ID', 'Offense Start DateTime', 'Offense End DateTime',\n",
       "       'Group A B', 'Crime Against Category', 'Offense Parent Group',\n",
       "       'Offense', 'Offense Code', 'Precinct', 'Sector', 'Beat', 'MCPP',\n",
       "       '100 Block Address', 'Longitude', 'Latitude', 'Year', 'Month', 'Day',\n",
       "       'Time', 'long_rad', 'lat_rad'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap_hill_23.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time window extraction needs debugging (endpoint issues due to rounding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 2022), (6, 2023)), ((7, 2022), (0, 2024))]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract time window as a series of month, year intervals\n",
    "def to_month_year(months, base_year):\n",
    "    year = base_year + int(months/ 12)\n",
    "    month = (months % 12)\n",
    "    return month, year\n",
    "\n",
    "\n",
    "def get_windows(start_year, end_year, length=18, step=6):\n",
    "    \"\"\"Generates a sequence of cuts as a 'sliding time window' \n",
    "    \"\"\"\n",
    "    num_months = (end_year - start_year) * 12\n",
    "    starts = range(1, num_months, step)\n",
    "    window_in_months = [(start, start + length - 1) for start in starts]\n",
    "    windows = [(to_month_year(window[0], start_year), to_month_year(window[1], start_year)) for window in window_in_months]\n",
    "    return windows\n",
    "\n",
    "def extract_windows(df, windows):\n",
    "    \"\"\"Temp solution. Assumes window will not span more than 2 years.\"\"\"\n",
    "    extracts = []\n",
    "    for window in windows:\n",
    "        start, end = window\n",
    "        # Get crimes in first year\n",
    "        df_filter1 = df.loc[(df[\"Year\"] == start[1]) & (df[\"Month\"] >= start[0])]\n",
    "        # Get crimes in the second year\n",
    "        df_filter2 = df.loc[(df[\"Year\"] == end[1]) & (df[\"Month\"] <= end[0])]\n",
    "        # Concat to windowed df\n",
    "        df_window = pd.concat([df_filter1, df_filter2], axis=0)\n",
    "        extracts.append((window, df_window))\n",
    "    return extracts\n",
    "\n",
    "# Need to join the endpoints into time ranges\n",
    "windows = get_windows(2022, 2023, length=18, step=6)\n",
    "extracts = extract_windows(crime_df.loc[crime_df[\"MCPP\"] == \"CAPITOL HILL\"], windows)\n",
    "windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually extract windows for now\n",
    "cap_hill = crime_df.loc[crime_df[\"MCPP\"] == \"CAPITOL HILL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter1 = cap_hill.loc[cap_hill[\"Year\"] == 2022]\n",
    "filter2 = cap_hill.loc[(cap_hill[\"Year\"] == 2023) & (cap_hill[\"Month\"] <= 6)]\n",
    "window1 = pd.concat([filter1, filter2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter3 = cap_hill.loc[cap_hill[\"Year\"] == 2022 & (cap_hill[\"Month\"] >= 7)]\n",
    "filter4 = cap_hill.loc[cap_hill[\"Year\"] == 2023]\n",
    "window2 = pd.concat([filter3, filter4], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBSCAN clustering with eps=75m, min_samples=139\n",
      "Estimated number of clusters: 12\n",
      "Estimated number of noise points: 3637\n",
      "Estimated percentage of crime captured: 0.3880195187615682\n",
      "\n",
      "DBSCAN clustering with eps=75m, min_samples=120\n",
      "Estimated number of clusters: 14\n",
      "Estimated number of noise points: 3108\n",
      "Estimated percentage of crime captured: 0.47703180212014135\n",
      "\n",
      "DBSCAN clustering with eps=75m, min_samples=94\n",
      "Estimated number of clusters: 22\n",
      "Estimated number of noise points: 2429\n",
      "Estimated percentage of crime captured: 0.591283863368669\n",
      "\n",
      "DBSCAN clustering with eps=75m, min_samples=83\n",
      "Estimated number of clusters: 24\n",
      "Estimated number of noise points: 2191\n",
      "Estimated percentage of crime captured: 0.6313309776207303\n",
      "\n",
      "DBSCAN clustering with eps=75m, min_samples=77\n",
      "Estimated number of clusters: 22\n",
      "Estimated number of noise points: 2109\n",
      "Estimated percentage of crime captured: 0.6451287228672388\n",
      "\n",
      "DBSCAN clustering with eps=75m, min_samples=75\n",
      "Estimated number of clusters: 18\n",
      "Estimated number of noise points: 2078\n",
      "Estimated percentage of crime captured: 0.6503449436311627\n",
      "\n",
      "DBSCAN clustering with eps=75m, min_samples=68\n",
      "Estimated number of clusters: 14\n",
      "Estimated number of noise points: 1834\n",
      "Estimated percentage of crime captured: 0.6914016489988222\n",
      "\n",
      "OPTICS clustering with max_eps=75m, min_samples=139\n",
      "Estimated number of clusters: 12\n",
      "Estimated number of noise points: 3752\n",
      "Estimated percentage of crime captured: 0.36866902237926974\n",
      "\n",
      "OPTICS clustering with max_eps=75m, min_samples=120\n",
      "Estimated number of clusters: 14\n",
      "Estimated number of noise points: 3226\n",
      "Estimated percentage of crime captured: 0.45717651018004374\n",
      "\n",
      "OPTICS clustering with max_eps=75m, min_samples=94\n",
      "Estimated number of clusters: 22\n",
      "Estimated number of noise points: 2648\n",
      "Estimated percentage of crime captured: 0.5544337876493354\n",
      "\n",
      "OPTICS clustering with max_eps=75m, min_samples=83\n",
      "Estimated number of clusters: 24\n",
      "Estimated number of noise points: 2414\n",
      "Estimated percentage of crime captured: 0.5938078411576645\n",
      "\n",
      "OPTICS clustering with max_eps=75m, min_samples=77\n",
      "Estimated number of clusters: 22\n",
      "Estimated number of noise points: 2254\n",
      "Estimated percentage of crime captured: 0.6207302709069493\n",
      "\n",
      "OPTICS clustering with max_eps=75m, min_samples=75\n",
      "Estimated number of clusters: 18\n",
      "Estimated number of noise points: 2158\n",
      "Estimated percentage of crime captured: 0.6368837287565203\n",
      "\n",
      "OPTICS clustering with max_eps=75m, min_samples=68\n",
      "Estimated number of clusters: 14\n",
      "Estimated number of noise points: 1901\n",
      "Estimated percentage of crime captured: 0.6801278815413091\n",
      "\n",
      "HDBSCAN clustering with max_eps=50m, min_samples=139\n",
      "Estimated number of clusters: 6\n",
      "Estimated number of noise points: 3535\n",
      "Estimated percentage of crime captured: 0.40518256772673733\n",
      "\n",
      "HDBSCAN clustering with max_eps=50m, min_samples=120\n",
      "Estimated number of clusters: 7\n",
      "Estimated number of noise points: 3374\n",
      "Estimated percentage of crime captured: 0.43227326266195526\n",
      "\n",
      "HDBSCAN clustering with max_eps=50m, min_samples=94\n",
      "Estimated number of clusters: 7\n",
      "Estimated number of noise points: 3772\n",
      "Estimated percentage of crime captured: 0.3653037186606091\n",
      "\n",
      "HDBSCAN clustering with max_eps=50m, min_samples=83\n",
      "Estimated number of clusters: 8\n",
      "Estimated number of noise points: 3551\n",
      "Estimated percentage of crime captured: 0.40249032475180885\n",
      "\n",
      "HDBSCAN clustering with max_eps=50m, min_samples=77\n",
      "Estimated number of clusters: 7\n",
      "Estimated number of noise points: 3537\n",
      "Estimated percentage of crime captured: 0.4048460373548713\n",
      "\n",
      "HDBSCAN clustering with max_eps=50m, min_samples=75\n",
      "Estimated number of clusters: 6\n",
      "Estimated number of noise points: 3557\n",
      "Estimated percentage of crime captured: 0.4014807336362107\n",
      "\n",
      "HDBSCAN clustering with max_eps=50m, min_samples=68\n",
      "Estimated number of clusters: 8\n",
      "Estimated number of noise points: 3825\n",
      "Estimated percentage of crime captured: 0.3563856638061585\n",
      "\n",
      "DBSCAN clustering with eps=75m, min_samples=72\n",
      "Estimated number of clusters: 13\n",
      "Estimated number of noise points: 2286\n",
      "Estimated percentage of crime captured: 0.39889560872995006\n",
      "\n",
      "DBSCAN clustering with eps=75m, min_samples=65\n",
      "Estimated number of clusters: 17\n",
      "Estimated number of noise points: 1973\n",
      "Estimated percentage of crime captured: 0.4811990533789114\n",
      "\n",
      "DBSCAN clustering with eps=75m, min_samples=62\n",
      "Estimated number of clusters: 17\n",
      "Estimated number of noise points: 1855\n",
      "Estimated percentage of crime captured: 0.5122271890612674\n",
      "\n",
      "DBSCAN clustering with eps=75m, min_samples=61\n",
      "Estimated number of clusters: 18\n",
      "Estimated number of noise points: 1848\n",
      "Estimated percentage of crime captured: 0.5140678411780174\n",
      "\n",
      "DBSCAN clustering with eps=75m, min_samples=55\n",
      "Estimated number of clusters: 20\n",
      "Estimated number of noise points: 1611\n",
      "Estimated percentage of crime captured: 0.5763870628451223\n",
      "\n",
      "DBSCAN clustering with eps=75m, min_samples=47\n",
      "Estimated number of clusters: 21\n",
      "Estimated number of noise points: 1341\n",
      "Estimated percentage of crime captured: 0.6473836444911911\n",
      "\n",
      "DBSCAN clustering with eps=75m, min_samples=45\n",
      "Estimated number of clusters: 23\n",
      "Estimated number of noise points: 1206\n",
      "Estimated percentage of crime captured: 0.6828819353142256\n",
      "\n",
      "DBSCAN clustering with eps=75m, min_samples=43\n",
      "Estimated number of clusters: 22\n",
      "Estimated number of noise points: 1157\n",
      "Estimated percentage of crime captured: 0.6957665001314751\n",
      "\n",
      "DBSCAN clustering with eps=75m, min_samples=41\n",
      "Estimated number of clusters: 25\n",
      "Estimated number of noise points: 1041\n",
      "Estimated percentage of crime captured: 0.7262687352090454\n",
      "\n",
      "DBSCAN clustering with eps=75m, min_samples=38\n",
      "Estimated number of clusters: 24\n",
      "Estimated number of noise points: 966\n",
      "Estimated percentage of crime captured: 0.7459900078885091\n",
      "\n",
      "OPTICS clustering with max_eps=75m, min_samples=72\n",
      "Estimated number of clusters: 13\n",
      "Estimated number of noise points: 2342\n",
      "Estimated percentage of crime captured: 0.3841703917959506\n",
      "\n",
      "OPTICS clustering with max_eps=75m, min_samples=65\n",
      "Estimated number of clusters: 17\n",
      "Estimated number of noise points: 2019\n",
      "Estimated percentage of crime captured: 0.46910333946884036\n",
      "\n",
      "OPTICS clustering with max_eps=75m, min_samples=62\n",
      "Estimated number of clusters: 17\n",
      "Estimated number of noise points: 1932\n",
      "Estimated percentage of crime captured: 0.49198001577701816\n",
      "\n",
      "OPTICS clustering with max_eps=75m, min_samples=61\n",
      "Estimated number of clusters: 18\n",
      "Estimated number of noise points: 1925\n",
      "Estimated percentage of crime captured: 0.49382066789376805\n",
      "\n",
      "OPTICS clustering with max_eps=75m, min_samples=55\n",
      "Estimated number of clusters: 20\n",
      "Estimated number of noise points: 1715\n",
      "Estimated percentage of crime captured: 0.5490402313962661\n",
      "\n",
      "OPTICS clustering with max_eps=75m, min_samples=47\n",
      "Estimated number of clusters: 21\n",
      "Estimated number of noise points: 1509\n",
      "Estimated percentage of crime captured: 0.6032079936891928\n",
      "\n",
      "OPTICS clustering with max_eps=75m, min_samples=45\n",
      "Estimated number of clusters: 23\n",
      "Estimated number of noise points: 1380\n",
      "Estimated percentage of crime captured: 0.6371285826978701\n",
      "\n",
      "OPTICS clustering with max_eps=75m, min_samples=43\n",
      "Estimated number of clusters: 22\n",
      "Estimated number of noise points: 1307\n",
      "Estimated percentage of crime captured: 0.656323954772548\n",
      "\n",
      "OPTICS clustering with max_eps=75m, min_samples=41\n",
      "Estimated number of clusters: 25\n",
      "Estimated number of noise points: 1166\n",
      "Estimated percentage of crime captured: 0.6933999474099395\n",
      "\n",
      "OPTICS clustering with max_eps=75m, min_samples=38\n",
      "Estimated number of clusters: 24\n",
      "Estimated number of noise points: 1054\n",
      "Estimated percentage of crime captured: 0.7228503812779384\n",
      "\n",
      "HDBSCAN clustering with max_eps=50m, min_samples=72\n",
      "Estimated number of clusters: 7\n",
      "Estimated number of noise points: 2332\n",
      "Estimated percentage of crime captured: 0.38679989481987903\n",
      "\n",
      "HDBSCAN clustering with max_eps=50m, min_samples=65\n",
      "Estimated number of clusters: 6\n",
      "Estimated number of noise points: 2405\n",
      "Estimated percentage of crime captured: 0.36760452274520117\n",
      "\n",
      "HDBSCAN clustering with max_eps=50m, min_samples=62\n",
      "Estimated number of clusters: 5\n",
      "Estimated number of noise points: 2121\n",
      "Estimated percentage of crime captured: 0.4422824086247699\n",
      "\n",
      "HDBSCAN clustering with max_eps=50m, min_samples=61\n",
      "Estimated number of clusters: 7\n",
      "Estimated number of noise points: 2272\n",
      "Estimated percentage of crime captured: 0.4025769129634499\n",
      "\n",
      "HDBSCAN clustering with max_eps=50m, min_samples=55\n",
      "Estimated number of clusters: 7\n",
      "Estimated number of noise points: 2340\n",
      "Estimated percentage of crime captured: 0.3846962924007363\n",
      "\n",
      "HDBSCAN clustering with max_eps=50m, min_samples=47\n",
      "Estimated number of clusters: 8\n",
      "Estimated number of noise points: 2263\n",
      "Estimated percentage of crime captured: 0.4049434656849855\n",
      "\n",
      "HDBSCAN clustering with max_eps=50m, min_samples=45\n",
      "Estimated number of clusters: 8\n",
      "Estimated number of noise points: 2434\n",
      "Estimated percentage of crime captured: 0.3599789639758086\n",
      "\n",
      "HDBSCAN clustering with max_eps=50m, min_samples=43\n",
      "Estimated number of clusters: 7\n",
      "Estimated number of noise points: 2353\n",
      "Estimated percentage of crime captured: 0.38127793846962926\n",
      "\n",
      "HDBSCAN clustering with max_eps=50m, min_samples=41\n",
      "Estimated number of clusters: 7\n",
      "Estimated number of noise points: 2373\n",
      "Estimated percentage of crime captured: 0.3760189324217723\n",
      "\n",
      "HDBSCAN clustering with max_eps=50m, min_samples=38\n",
      "Estimated number of clusters: 9\n",
      "Estimated number of noise points: 2374\n",
      "Estimated percentage of crime captured: 0.37575598211937944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try some runs\n",
    "windows = [window1, window2]\n",
    "results = []\n",
    "summaries = []\n",
    "for i, window in enumerate(windows):\n",
    "    # Radians for haversine\n",
    "    X = window[[\"lat_rad\", \"long_rad\"]]\n",
    "    # Taking top N ensures these blocks will be core points\n",
    "    core_threshes = np.flip(np.unique(top_blocks(window, 10)))\n",
    "    # run db_scans\n",
    "    output_df, db_summaries = run_dbscans(X, window, core_threshes, eps_meters=75)\n",
    "    output_df, op_summaries = run_optics(X, output_df, core_threshes, max_eps_meters=75)\n",
    "    output_df, hdb_summaries = run_hdbscans(X, output_df, core_threshes, core_threshes[0], cluster_selection_eps_meters=50)\n",
    "    summaries.append((db_summaries, op_summaries, hdb_summaries))\n",
    "    results.append(output_df)\n",
    "    output_df.to_csv(f\"win{i}_cap_hill_clusters.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.715"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows[0].shape[0] * .005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBSCAN clustering with eps=75m, min_samples=169\n",
      "Estimated number of clusters: 6\n",
      "Estimated number of noise points: 4827\n",
      "Estimated percentage of crime captured: 0.18778394750126198\n",
      "\n",
      "DBSCAN clustering with eps=75m, min_samples=150\n",
      "Estimated number of clusters: 11\n",
      "Estimated number of noise points: 3898\n",
      "Estimated percentage of crime captured: 0.3441023052330473\n",
      "\n",
      "DBSCAN clustering with eps=75m, min_samples=124\n",
      "Estimated number of clusters: 15\n",
      "Estimated number of noise points: 3221\n",
      "Estimated percentage of crime captured: 0.4580178361097089\n",
      "\n",
      "DBSCAN clustering with eps=75m, min_samples=113\n",
      "Estimated number of clusters: 15\n",
      "Estimated number of noise points: 2963\n",
      "Estimated percentage of crime captured: 0.5014302540804307\n",
      "\n",
      "DBSCAN clustering with eps=75m, min_samples=107\n",
      "Estimated number of clusters: 17\n",
      "Estimated number of noise points: 2877\n",
      "Estimated percentage of crime captured: 0.5159010600706714\n",
      "\n",
      "DBSCAN clustering with eps=75m, min_samples=105\n",
      "Estimated number of clusters: 18\n",
      "Estimated number of noise points: 2765\n",
      "Estimated percentage of crime captured: 0.5347467608951708\n",
      "\n",
      "DBSCAN clustering with eps=75m, min_samples=98\n",
      "Estimated number of clusters: 20\n",
      "Estimated number of noise points: 2596\n",
      "Estimated percentage of crime captured: 0.563183577317853\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df, summaries = run_dbscans(\n",
    "    windows[0][[\"lat_rad\", \"long_rad\"]],\n",
    "    windows[0],\n",
    "    core_threshes=np.flip(np.unique(top_blocks(windows[0], 10))) + 30,\n",
    "    eps_meters=75,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"test2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
