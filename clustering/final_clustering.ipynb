{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "\n",
    "# Important for conversion of lat-long based distance to meters\n",
    "RADIUS_OF_EARTH_AT_SPACE_NEEDLE = 6366.512563943 # km\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random helpers + custom distance metrics \n",
    "def extract_yearly_data(df, dirname, base_filename, year_range):\n",
    "    dirpath = os.path.join(os.getcwd(), dirname)\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "    path = os.path.join(dirpath, base_filename)\n",
    "    for year in year_range:\n",
    "        year_df = df.loc[df[\"Year\"] == year]\n",
    "        year_df.to_csv(path + \"_year_\" + str(year) + \".csv\", index=False)\n",
    "    return\n",
    "\n",
    "def top_block_counts(crime_df, n):\n",
    "    \"\"\"List in ascending order\"\"\"\n",
    "    top_blocks = (\n",
    "        crime_df.groupby([\"Latitude\", \"Longitude\"])\n",
    "        .size()\n",
    "        .to_frame(name=\"count\")\n",
    "        .reset_index()\n",
    "        .sort_values(by=[\"count\"], ascending=True)\n",
    "        .tail(n)\n",
    "    )\n",
    "    return top_blocks[\"count\"].values\n",
    "\n",
    "def meters_to_hav(meters, R=RADIUS_OF_EARTH_AT_SPACE_NEEDLE):\n",
    "    \"\"\"Converts a distance in meters to haversine distance\"\"\"\n",
    "    hav = meters / (R * 1000)\n",
    "    return hav\n",
    "\n",
    "def haversine_np(lon1, lat1, lon2, lat2, R=RADIUS_OF_EARTH_AT_SPACE_NEEDLE):\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = R * c\n",
    "    return km\n",
    "\n",
    "def linearized_haversine(a, b, R=RADIUS_OF_EARTH_AT_SPACE_NEEDLE):\n",
    "    a_rad = a * (2 * np.pi / 360)\n",
    "    b_rad = b * (2 * np.pi / 360)\n",
    "    x_1, y_1 = a_rad\n",
    "    x_2, y_2 = b_rad\n",
    "    delta_x = R * np.cos(y_1) * (x_2 - x_1)\n",
    "    delta_y = R * (y_2 - y_1)\n",
    "    return np.sqrt(delta_x**2 + delta_y**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows_months(start_year, end_year, length_months=18, step_months=6):\n",
    "    num_months = (end_year - start_year + 1) * 12\n",
    "    starts = range(1, num_months + 1, step_months)\n",
    "    ends = [min(start + length_months - 1, num_months) for start in starts]\n",
    "    windows_in_months = list(zip(starts, ends))\n",
    "    if length_months == step_months:\n",
    "        return windows_in_months\n",
    "    else:\n",
    "        return windows_in_months[: -(int(length_months / step_months) - 1)]\n",
    "\n",
    "def to_month_year(months, base_year):\n",
    "    year = base_year + int((months - 1) / 12)\n",
    "    month = ((months - 1) % 12) + 1\n",
    "    return month, year\n",
    "\n",
    "def get_windows(start_year, end_year, length_months=18, step_months=6):\n",
    "    windows_in_months = get_windows_months(\n",
    "        start_year, end_year, length_months=length_months, step_months=step_months\n",
    "    )\n",
    "    windows = [\n",
    "        (to_month_year(start, start_year), to_month_year(end, start_year))\n",
    "        for start, end in windows_in_months\n",
    "    ]\n",
    "    return windows\n",
    "\n",
    "def extract_windows(df, windows):\n",
    "    extracts = []\n",
    "    for window in windows:\n",
    "        start, end = window\n",
    "        start_month, start_year = start\n",
    "        end_month, end_year = end\n",
    "        # Get crimes in year range\n",
    "        df_years = df.loc[(df[\"Year\"] >= start_year) & (df[\"Year\"] <= end_year)]\n",
    "        # Remove crimes in start year out of range\n",
    "        df_filter1 = df_years.loc[\n",
    "            ~((df_years[\"Year\"] == start_year) & (df_years[\"Month\"] < start_month))\n",
    "        ]\n",
    "        # Remove crimes in end year out of range\n",
    "        df_window = df_filter1.loc[\n",
    "            ~((df_filter1[\"Year\"] == end_year) & (df_filter1[\"Month\"] > end_month))\n",
    "        ]\n",
    "        extracts.append((window, df_window))\n",
    "    return extracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_hdbscans(windows, alphas, max_size_mult, max_eps_m=150):\n",
    "    \"\"\"\n",
    "    Search for a decent, stable alpha for each n_hood -> df + summaries\n",
    "    Labels stored in output df. Need to write it to disk after for Tableau.\n",
    "    \"\"\"\n",
    "    run_summaries = defaultdict(list)\n",
    "    clustered_dfs = defaultdict(list)\n",
    "    max_eps_hav = meters_to_hav(max_eps_m)\n",
    "    # For each alpha, cluster each window\n",
    "    for alpha in alphas:\n",
    "        alpha = round(alpha, 5)\n",
    "        colname = \"cluster_labs\"\n",
    "        print(f\"Clustering with alpha={alpha}\")\n",
    "        for i, (_, df) in enumerate(windows):\n",
    "            output_df = df.copy()\n",
    "            X = output_df[[\"lat_rad\", \"long_rad\"]]\n",
    "            crime_in_window = X.shape[0]\n",
    "            min_samples = int(alpha * crime_in_window)\n",
    "            # HDBSCAN\n",
    "            hdb = HDBSCAN(\n",
    "                cluster_selection_epsilon=max_eps_hav,\n",
    "                min_samples=min_samples,\n",
    "                min_cluster_size=2 * min_samples,\n",
    "                max_cluster_size=int(max_size_mult) * min_samples,\n",
    "                metric=\"haversine\",\n",
    "                store_centers=\"centroid\",\n",
    "            ).fit(X)\n",
    "            labels_hdb_ = hdb.labels_\n",
    "            output_df[colname] = labels_hdb_\n",
    "            # Count clusters and noise\n",
    "            num_clusters_ = len(set(labels_hdb_)) - (1 if -1 in labels_hdb_ else 0)\n",
    "            num_noise_ = list(labels_hdb_).count(-1)\n",
    "            percent_clustered_ = (crime_in_window - num_noise_) / crime_in_window\n",
    "            # Store df and summary\n",
    "            run_summaries[alpha].append((i, num_clusters_, percent_clustered_))\n",
    "            clustered_dfs[alpha].append(output_df)\n",
    "    return run_summaries, clustered_dfs\n",
    "\n",
    "\n",
    "def grid_search_epsilons(windows, alpha, max_epsilons):\n",
    "    \"\"\"\n",
    "    given a fixed alpha, refine max_eps -> df + summaries\n",
    "    Labels stored in output df. Need to write it to disk for Tableau.\n",
    "    \"\"\"\n",
    "    run_summaries = defaultdict(list)\n",
    "    clustered_dfs = defaultdict(list)\n",
    "    alpha = round(alpha, 5)\n",
    "    # For each max_eps, cluster each window\n",
    "    for max_eps_m in max_epsilons:\n",
    "        colname = \"clust_eps=\" + str(max_eps_m)\n",
    "        print(f\"Clustering with max_eps={max_eps_m}\")\n",
    "        max_eps_hav = meters_to_hav(max_eps_m)\n",
    "        for i, (_, df) in enumerate(windows):\n",
    "            output_df = df.copy()\n",
    "            X = output_df[[\"lat_rad\", \"long_rad\"]]\n",
    "            crime_in_window = X.shape[0]\n",
    "            min_samples = int(alpha * crime_in_window)\n",
    "            # HDBSCAN\n",
    "            hdb = HDBSCAN(\n",
    "                cluster_selection_epsilon=max_eps_hav,\n",
    "                min_samples=min_samples,\n",
    "                min_cluster_size=min_samples,\n",
    "                metric=\"haversine\",\n",
    "                store_centers=\"centroid\",\n",
    "            ).fit(X)\n",
    "            labels_hdb_ = hdb.labels_\n",
    "            output_df[colname] = labels_hdb_\n",
    "            # Count clusters and noise\n",
    "            num_clusters_ = len(set(labels_hdb_)) - (1 if -1 in labels_hdb_ else 0)\n",
    "            num_noise_ = list(labels_hdb_).count(-1)\n",
    "            percent_clustered_ = (crime_in_window - num_noise_) / crime_in_window\n",
    "            # Store df and summary\n",
    "            run_summaries[max_eps_m].append((i, num_clusters_, percent_clustered_))\n",
    "            clustered_dfs[max_eps_m].append(output_df)\n",
    "    return run_summaries, clustered_dfs\n",
    "\n",
    "\n",
    "def plot_summaries_alphas(run_summaries):\n",
    "    num_plots = len(run_summaries)\n",
    "    fig, axes = plt.subplots(num_plots, 2, figsize=(36, 12))\n",
    "    axes = axes.flatten()  # Flatten the axes for easier iteration\n",
    "    for i, alpha in enumerate(run_summaries):\n",
    "        arr = np.array(run_summaries[alpha])\n",
    "        windows = arr[:, 0].astype(int)\n",
    "        cluster_counts = arr[:, 1]\n",
    "        percentages_clustered = arr[:, 2]\n",
    "        ax1, ax2 = axes[i * 2], axes[i * 2 + 1]  # Get subplots for current plot\n",
    "        # Plot nclusters vs. windows\n",
    "        ax1.plot(windows, cluster_counts)\n",
    "        ax1.set_xticks(windows)\n",
    "        ax1.set_xlabel(\"Window\")\n",
    "        ax1.set_ylabel(\"Number of Clusters\")\n",
    "        ax1.set_title(f\"Cluster counts by window: alpha={alpha}\")\n",
    "        # Plot percent_clustered vs. windows\n",
    "        ax2.plot(windows, percentages_clustered)\n",
    "        ax2.set_xticks(windows)\n",
    "        ax2.set_xlabel(\"Window\")\n",
    "        ax2.set_ylabel(\"% Crime Clustered\")\n",
    "        ax2.set_title(f\"% Clustered by window: alpha={alpha}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "def plot_summaries_eps(run_summaries):\n",
    "    num_plots = len(run_summaries)\n",
    "    fig, axes = plt.subplots(num_plots, 2, figsize=(24, 12))\n",
    "    axes = axes.flatten()  # Flatten the axes for easier iteration\n",
    "    for i, eps in enumerate(run_summaries):\n",
    "        arr = np.array(run_summaries[eps])\n",
    "        windows = arr[:, 0].astype(int)\n",
    "        cluster_counts = arr[:, 1]\n",
    "        percentages_clustered = arr[:, 2]\n",
    "        ax1, ax2 = axes[i * 2], axes[i * 2 + 1]  # Get subplots for current plot\n",
    "        # Plot nclusters vs. windows\n",
    "        ax1.plot(windows, cluster_counts)\n",
    "        ax1.set_xticks(windows)\n",
    "        ax1.set_xlabel(\"Window\")\n",
    "        ax1.set_ylabel(\"Number of Clusters\")\n",
    "        ax1.set_title(f\"Cluster counts by window: eps={eps}\")\n",
    "        # Plot percent_clustered vs. windows\n",
    "        ax2.plot(windows, percentages_clustered)\n",
    "        ax2.set_xticks(windows)\n",
    "        ax2.set_xlabel(\"Window\")\n",
    "        ax2.set_ylabel(\"% Crime Clustered\")\n",
    "        ax2.set_title(f\"% Clustered by window: eps={eps}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_all_neighborhoods_alpha(\n",
    "    crime_df, sectors_to_cluster, window_times, alphas, max_eps_m=100\n",
    "):\n",
    "    all_summaries = {}\n",
    "    all_dfs = {}\n",
    "    for sector in sectors_to_cluster:\n",
    "        print(f\"Clustering sector {sector}\")\n",
    "        df = crime_df.loc[crime_df[\"Sector\"] == sector]\n",
    "        windows = extract_windows(df, window_times)\n",
    "        run_summaries, clustered_dfs = grid_search_hdbscans(\n",
    "            windows, alphas, max_eps_m=max_eps_m\n",
    "        )\n",
    "        all_summaries[sector] = run_summaries\n",
    "        all_dfs[sector] = clustered_dfs\n",
    "    return all_summaries, all_dfs\n",
    "\n",
    "\n",
    "def grid_search_all_neighborhoods_eps(\n",
    "    crime_df, sectors_to_cluster, window_times, best_alphas, epsilons\n",
    "):\n",
    "    all_summaries = {}\n",
    "    all_dfs = {}\n",
    "    for sector in sectors_to_cluster:\n",
    "        alpha = best_alphas[sector]\n",
    "        print(f\"Clustering sector {sector}\")\n",
    "        df = crime_df.loc[crime_df[\"Sector\"] == sector]\n",
    "        windows = extract_windows(df, window_times)\n",
    "        run_summaries, clustered_dfs = grid_search_epsilons(\n",
    "            windows, alpha, epsilons\n",
    "        )\n",
    "        all_summaries[sector] = run_summaries\n",
    "        all_dfs[sector] = clustered_dfs\n",
    "    return all_summaries, all_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the cleaned data\n",
    "crime_df = pd.read_csv(\"../../cleaned_SPD_Crime_Data.csv\")\n",
    "crime_df = crime_df.loc[crime_df[\"Year\"] < 2024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['U', 'B', 'E', 'M', 'K', 'D', 'Q', 'R', 'L', 'N', 'J', 'W', 'S',\n",
       "       'F', 'C', 'G', 'O'], dtype=object)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the relevant clusters\n",
    "sectors = crime_df[\"Sector\"].value_counts().index\n",
    "sectors_to_cluster = sectors.values[:17]\n",
    "sectors_to_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster year by year for now\n",
    "window_times = get_windows(2008, 2023, length_months=12, step_months=12)\n",
    "# alphas = np.linspace(0.005, 0.03, 11)  # alphas to test\n",
    "# all_summaries, all_dfs = grid_search_all_neighborhoods_alpha(\n",
    "#     crime_df, sectors_to_cluster, window_times, alphas\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can check the run summaries to choose alphas by neighborhood\n",
    "# run_summaries = all_summaries['O']\n",
    "# plot_summaries_alphas(run_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can write individual windows to disk here\n",
    "# clustered_dfs_alphas = all_dfs['K']\n",
    "# clustered_dfs_alphas[.02][0].to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These are the alphas that resulted in the most stable small clusters across all windows.\n",
    "Unfortunately, even with these alphas there are still windows with degenrate clusters.\n",
    "I plan to manually fix these bad clusterings by setting a different alpha/eps for each\n",
    "problem window, as needed. The degree to which I'll focus on that will depend\n",
    "on how important those windows end up being to our analysis.\n",
    "\"\"\"\n",
    "\n",
    "my_favorite_alphas = {\n",
    "    \"U\": 0.015,\n",
    "    \"B\": 0.01,\n",
    "    \"E\": 0.025,\n",
    "    \"M\": 0.03,\n",
    "    \"K\": 0.025,\n",
    "    \"D\": 0.015,\n",
    "    \"Q\": 0.015,\n",
    "    \"R\": 0.025,\n",
    "    \"L\": 0.02,\n",
    "    \"N\": 0.0225,\n",
    "    \"J\": 0.02,\n",
    "    \"W\": 0.02,\n",
    "    \"S\": 0.03,\n",
    "    \"F\": 0.02,\n",
    "    \"C\": 0.015,\n",
    "    \"G\": 0.02,\n",
    "    \"O\": 0.025,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sector U\n",
      "Clustering with max_eps=50\n",
      "Clustering with max_eps=75\n",
      "Clustering with max_eps=100\n",
      "Clustering sector B\n",
      "Clustering with max_eps=50\n",
      "Clustering with max_eps=75\n",
      "Clustering with max_eps=100\n",
      "Clustering sector E\n",
      "Clustering with max_eps=50\n",
      "Clustering with max_eps=75\n",
      "Clustering with max_eps=100\n",
      "Clustering sector M\n",
      "Clustering with max_eps=50\n",
      "Clustering with max_eps=75\n",
      "Clustering with max_eps=100\n",
      "Clustering sector K\n",
      "Clustering with max_eps=50\n",
      "Clustering with max_eps=75\n",
      "Clustering with max_eps=100\n",
      "Clustering sector D\n",
      "Clustering with max_eps=50\n",
      "Clustering with max_eps=75\n",
      "Clustering with max_eps=100\n",
      "Clustering sector Q\n",
      "Clustering with max_eps=50\n",
      "Clustering with max_eps=75\n",
      "Clustering with max_eps=100\n",
      "Clustering sector R\n",
      "Clustering with max_eps=50\n",
      "Clustering with max_eps=75\n",
      "Clustering with max_eps=100\n",
      "Clustering sector L\n",
      "Clustering with max_eps=50\n",
      "Clustering with max_eps=75\n",
      "Clustering with max_eps=100\n",
      "Clustering sector N\n",
      "Clustering with max_eps=50\n",
      "Clustering with max_eps=75\n",
      "Clustering with max_eps=100\n",
      "Clustering sector J\n",
      "Clustering with max_eps=50\n",
      "Clustering with max_eps=75\n",
      "Clustering with max_eps=100\n",
      "Clustering sector W\n",
      "Clustering with max_eps=50\n",
      "Clustering with max_eps=75\n",
      "Clustering with max_eps=100\n",
      "Clustering sector S\n",
      "Clustering with max_eps=50\n",
      "Clustering with max_eps=75\n",
      "Clustering with max_eps=100\n",
      "Clustering sector F\n",
      "Clustering with max_eps=50\n",
      "Clustering with max_eps=75\n",
      "Clustering with max_eps=100\n",
      "Clustering sector C\n",
      "Clustering with max_eps=50\n",
      "Clustering with max_eps=75\n",
      "Clustering with max_eps=100\n",
      "Clustering sector G\n",
      "Clustering with max_eps=50\n",
      "Clustering with max_eps=75\n",
      "Clustering with max_eps=100\n",
      "Clustering sector O\n",
      "Clustering with max_eps=50\n",
      "Clustering with max_eps=75\n",
      "Clustering with max_eps=100\n"
     ]
    }
   ],
   "source": [
    "# Try restricting epsilon to reduce number of big clusters\n",
    "# max_epsilons = [50, 75, 100]\n",
    "# all_summaries_eps, all_dfs_eps = grid_search_all_neighborhoods_eps(\n",
    "#     crime_df, sectors_to_cluster, window_times, my_favorite_alphas, max_epsilons\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can check here for epsilon selection after alpha is chosen\n",
    "# run_summaries_eps = all_summaries_eps['O']\n",
    "# plot_summaries_eps(run_summaries_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can write output by sector here\n",
    "# clustered_dfs_eps = all_dfs_eps['E']\n",
    "# clustered_dfs_eps[50][0].to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These are the 'best' alpha/epsilon combinations for each window. Dialing\n",
    "back the max_eps oftentimes did not affect cluster stability, but\n",
    "sometimes it did actually improve it.\n",
    "\"\"\"\n",
    "\n",
    "best_params = {\n",
    "    \"U\": (0.015, 100, 7),\n",
    "    \"B\": (0.01, 100, 10),\n",
    "    \"E\": (0.025, 75, 10),\n",
    "    \"M\": (0.02, 65, 50),\n",
    "    \"K\": (0.025, 75, 10),\n",
    "    \"D\": (0.0125, 75, 17),\n",
    "    \"Q\": (0.01, 11, 30),\n",
    "    \"R\": (0.015, 20, 15),\n",
    "    \"L\": (0.02, 50, 10),\n",
    "    \"N\": (0.0225, 50, 10),\n",
    "    \"J\": (0.02, 50, 10),\n",
    "    \"W\": (0.015, 40, 30),\n",
    "    \"S\": (0.02, 50, 30),\n",
    "    \"F\": (0.02, 50, 10),\n",
    "    \"C\": (0.015, 50, 10),\n",
    "    \"G\": (0.02, 75, 10),\n",
    "    \"O\": (0.025, 50, 10),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_all_neighborhoods(crime_df, sectors_to_cluster, window_times, best_params):\n",
    "    all_summaries = {}\n",
    "    all_dfs = {}\n",
    "    for sector in sectors_to_cluster:\n",
    "        alpha, max_eps_m, max_size_mult = best_params[sector]\n",
    "        print(f\"Clustering sector {sector} with alpha={alpha} and max_eps={max_eps_m}\")\n",
    "        df = crime_df.loc[crime_df[\"Sector\"] == sector]\n",
    "        windows = extract_windows(df, window_times)\n",
    "        run_summaries, clustered_dfs = grid_search_hdbscans(\n",
    "            windows, [alpha], max_size_mult, max_eps_m=max_eps_m\n",
    "        )\n",
    "        all_summaries[sector] = run_summaries\n",
    "        all_dfs[sector] = clustered_dfs\n",
    "    return all_summaries, all_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sector U with alpha=0.015 and max_eps=100\n",
      "Clustering with alpha=0.015\n",
      "Clustering sector B with alpha=0.01 and max_eps=100\n",
      "Clustering with alpha=0.01\n",
      "Clustering sector E with alpha=0.025 and max_eps=75\n",
      "Clustering with alpha=0.025\n",
      "Clustering sector M with alpha=0.02 and max_eps=65\n",
      "Clustering with alpha=0.02\n",
      "Clustering sector K with alpha=0.025 and max_eps=75\n",
      "Clustering with alpha=0.025\n",
      "Clustering sector D with alpha=0.0125 and max_eps=75\n",
      "Clustering with alpha=0.0125\n",
      "Clustering sector Q with alpha=0.01 and max_eps=11\n",
      "Clustering with alpha=0.01\n",
      "Clustering sector R with alpha=0.015 and max_eps=20\n",
      "Clustering with alpha=0.015\n",
      "Clustering sector L with alpha=0.02 and max_eps=50\n",
      "Clustering with alpha=0.02\n",
      "Clustering sector N with alpha=0.0225 and max_eps=50\n",
      "Clustering with alpha=0.0225\n",
      "Clustering sector J with alpha=0.02 and max_eps=50\n",
      "Clustering with alpha=0.02\n",
      "Clustering sector W with alpha=0.015 and max_eps=40\n",
      "Clustering with alpha=0.015\n",
      "Clustering sector S with alpha=0.02 and max_eps=50\n",
      "Clustering with alpha=0.02\n",
      "Clustering sector F with alpha=0.02 and max_eps=50\n",
      "Clustering with alpha=0.02\n",
      "Clustering sector C with alpha=0.015 and max_eps=50\n",
      "Clustering with alpha=0.015\n",
      "Clustering sector G with alpha=0.02 and max_eps=75\n",
      "Clustering with alpha=0.02\n",
      "Clustering sector O with alpha=0.025 and max_eps=50\n",
      "Clustering with alpha=0.025\n"
     ]
    }
   ],
   "source": [
    "all_summaries, all_dfs = cluster_all_neighborhoods(\n",
    "    crime_df, sectors_to_cluster, window_times, best_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_results(all_dfs, best_params):\n",
    "    by_neighborhood = []\n",
    "    for sector in all_dfs:\n",
    "        alpha, _, _= best_params[sector]\n",
    "        clustered_windows = all_dfs[sector][alpha]\n",
    "        joined_df = pd.concat(clustered_windows, axis=0)\n",
    "        by_neighborhood.append(joined_df)\n",
    "    all_neighborhoods = pd.concat(by_neighborhood, axis=0)\n",
    "    return all_neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_neighborhoods = concatenate_results(all_dfs, best_params)\n",
    "all_neighborhoods.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Began reclustering here\n",
    "Some windows, even after tuning, had degenerate clusters. We try to improve those clusters here. Post-processing will take care of some of the problems, but not all of them, so we it's worth it to create some better raw materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read output from previous cluster\n",
    "# all_neighborhoods = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the sectors that had windows with degenerate clusters \n",
    "# sectors_to_fix = [\"B\", \"C\", \"D\", \"M\", \"N\", \"O\", \"Q\", \"W\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n",
      "Clustering with alpha=0.02\n",
      "Clustering with alpha=0.0225\n",
      "Clustering with alpha=0.025\n",
      "Clustering with alpha=0.0275\n",
      "Clustering with alpha=0.03\n",
      "Clustering with alpha=0.0325\n",
      "Clustering with alpha=0.035\n",
      "Clustering with alpha=0.0375\n",
      "Clustering with alpha=0.04\n"
     ]
    }
   ],
   "source": [
    "# Our approach is manual. We select new alphas/eps, then rerun just on that sector\n",
    "def recluster_sector(sector, crime_df, window_times, best_params):\n",
    "    _, recluster_dfs = cluster_all_neighborhoods(\n",
    "        crime_df, [sector], window_times, best_params\n",
    "    )\n",
    "    recluster_df = concatenate_results(recluster_dfs, best_params)\n",
    "    recluster_df.to_csv(\"test.csv\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recluster_sector(\"W\", crime_df, window_times, best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
